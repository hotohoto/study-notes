# Layer Normalization

- https://arxiv.org/abs/1607.06450

## 1 Introduction

## 2 Background

## 3 Layer normalization

## 4 Related work

## 5 Analysis

### 5.1 Invariance under weights and data transformations

##### Weight re-scaling and re-centering

### 5.2 Geometry of parameter space during learning

#### 5.2.1 Riemannian metric

#### 5.2.2 The geometry of normalized generalized linear models

##### Implicit learning rate reduction through the growth of the weight vector

##### Learning the magnitude of incoming weights

## 6 Experimental results

### 6.1 Order embeddings of images and language

### 6.2  Teaching machines to read and comprehend

### 6.3 Skip-thought vectors

### 6.4 Modeling binarized MNIST using DRAW

### 6.5 Handwriting sequence generation

### 6.6 Permutation invariant MNIST

### 6.7 Convolutional networks

## 7 Conclusion

## References

## Supplementary material

##### Application of layer normalization to each experiment

##### Teaching machines to read and comprehend and handwriting sequence generation

##### Order embeddings and skip-thoughts

##### Modeling binarized MNIST using DRAW

##### Learning the magnitude of incoming weights

