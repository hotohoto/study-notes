# PRML

## Personal Questions

TODO

(1.1)

- how to interpret over-fitting from the perspective of MLE
- By adopting a Bayesian approach, the over-fitting problem can be avoided. ??
  - We shall see that there is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points.
  - Indeed, in a Bayesian model the effective number of parameters adapts automatically to the size of the data set.

## ğŸ¢ 1 Introduction

- generalization
  - The ability to categorize correctly new examples that differ from those used for training
- unsupervised learning
  - clustering
  - density estimation
  - dimension reduction
    - useful for visualization
- useful studies
  - probability theory
  - decision theory
  - information theory

### ğŸ¢ 1.1 Example: Polynomial Curve Fitting

- RMS error
  - $E_\text{RMS} = \sqrt{2 E(w^*) / N} = \sqrt{ \sum\limits_{n=1}^N 2 \times {1 \over 2N} (y(x_n, w^*) - t_n)^2 }$
- shrinkage methods (in statistics literatures)
  - weight decay (in the context of neural nets)
  - examples
    - ridge regression
    - lasso
- training data
  - training set
    - used to determine weights
  - validation set
    - also called a hold-out set
    - used to determine hyperparameters

### ğŸ¢ 1.2 Probability Theory

- random variable
- sum rule
- product rule
- joint probability
- marginal probability
- conditional probability
- Bayes' theorem
  - prior probability
  - posterior probability
- independent random variables

#### ğŸ¢ 1.2.1 Probability densities

- probability density
  - can be seen as derivative of cumulative distribution function
- change of variable of probability density function using the Jacobian factor
- cumulative distribution function
- joint probability density

#### ğŸ¢ 1.2.2 Expectations and covariances

- expectation
- conditional expectation
- variance
- covariance
  - of 2 random variables
- covariance matrix
  - of 2 random vectors

#### ğŸ¢ 1.2.3 Bayesian probabilities

- frequentist
  - considers
    - model parameters as fixed
      - MLE finds the best parameters(estimates) as a fixed value
    - there are multiple datasets
      - bootstrap is used to obtain error bars on the estimates
        - by considering the distribution of possible data sets
- Bayesian
  - considers
    - model parameters as a random variable
      - there is uncertainty
    - there is a single observed dataset
  - pros
    - we can be less extreme when number of samples are small
  - cons
    - not effective if a prior is poorly chosen
      - usually just for convenience
    - the evidence term is difficult to calculate
      - MCMC and high computing power addressed this
        - and opened the door to Bayesian methods
  - MCMC
    - very flexible
      - can be applied to a wide range of models
    - but computationally intensive
    - alternatives
      - variational Bayes
      - expectation propagation

#### ğŸ¢ 1.2.4 The Gaussian distribution

- Under the normal distribution assumption, MLE will fit like the below.
  - $\mu_\text{ML} = {1 \over N}\sum\limits_{n=1}^{N}x_n$
  - $\sigma_\text{ML} = {1 \over N}\sum\limits_{n=1}^{N}(x_n - \mu_\text{ML})^2$
    - This is the biased sample variance that underestimates the variance.
    - N may not be a big problem when it's a large number.
    - This will lead to overfitting since it's not fitting with respect to the true $\mu$

#### ğŸ¢ 1.2.5 Curve fitting re-visited

- $p(\mathbf{w}|\mathsf{x}, \mathsf{t}, \alpha, \beta) \propto p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta)p(\mathbf{w}|\alpha)$

- $
\argmax\limits_{w} p(\mathbf{w}|\mathsf{x}, \mathsf{t}, \alpha, \beta) \\
= \argmax\limits_{w} p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta)p(\mathbf{w}|\alpha) \\
= \argmax\limits_{w} [\log p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta) + \log p(\mathbf{w}|\alpha)] \\
= \argmin\limits_{w} [ - \log p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta) - \log p(\mathbf{w}|\alpha)] \\
= \argmin\limits_{w} [ {\beta \over 2} \sum\limits_{n=1}^{N} \{y(x_n, \mathbf{w}) - t_n\}^2 + {\alpha \over 2} \mathbf{w}^T\mathbf{w}] \\
= \argmin\limits_{w} [ \sum\limits_{n=1}^{N} \{y(x_n, \mathbf{w}) - t_n\}^2 + \lambda  \mathbf{w}^T\mathbf{w}] \\
$
- assuming
  - $p(t|x, \mathbf{w}, \beta) = \mathcal{N}(t|y(x|\mathbf{w}), \beta^{-1})$
  - $p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta) = \prod\limits_{n=1}^N \mathcal{N}(t_n|y(x_n|\mathbf{w}), \beta^{-1}) = \prod\limits_{n=1}^N ({\beta \over 2\pi})^{1/2}\exp(\beta(t_n - y(x_n|\mathbf{w}))^2)$
    - $\because$ i.i.d.
  - $p(\mathbf{w}| \alpha) = \mathcal{N}(\mathbf{w}| \mathbf{0}, \alpha^{-1}\mathbf{I}) = ({\alpha \over 2 \pi})^{(M+1)/2} \exp {\{- {\alpha \over 2} \mathbf{w}^T\mathbf{w}\}}$
- where
  - the training data is $\{\mathsf{x}, \mathsf{t}\}$
    - $\mathsf{x} = (x_1, ..., x_N)^T$
    - $\mathsf{t} = (t_1, ..., t_N)^T$
  - $\alpha$: precision of the prior distribution of $\mathbf{w}$
  - $\beta$: precision of the distribution of the target $t$
  - $M$: the order of the polynomial hypothesis function
  - $\lambda = \alpha /\beta$

#### ğŸ‡ 1.2.6 Bayesian curve fitting

$$
p(t|x, \mathsf{x}, \mathsf{t}) = \int p(t|x, \mathsf{w}) p(\mathbf{w}|\mathsf{x},\mathsf{t}) d\mathbf{w} \\
= \int
  p(t|x, \mathsf{w}, \beta) \
  {
    p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta)p(\mathbf{w}|\alpha)
    \over
    p(\mathsf{t}|\mathsf{x}, \beta)
  }
  d\mathbf{w} \\
= \int {
  p(t|x, \mathsf{w}, \beta) p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta)p(\mathbf{w}|\alpha)
  \over
  \int p(\mathsf{t}|\mathsf{x}, \mathbf{w}, \beta)p(\mathbf{w}|\alpha)d\mathbf{w}
} d\mathbf{w} \\
= \int {
  \mathcal{N}(t|y(x|\mathbf{w}), \beta^{-1}) \prod\limits_{n=1}^N \mathcal{N}(t_n|y(x_n|\mathbf{w}), \beta^{-1})\mathcal{N}(\mathbf{w}|\mathbf{0}, \alpha^{-1}\mathbf{I})
  \over
  \int \prod\limits_{n=1}^N \mathcal{N}(t_n|y(x_n|\mathbf{w}), \beta^{-1})\mathcal{N}(\mathbf{w}|\mathbf{0}, \alpha^{-1}\mathbf{I})d\mathbf{w}
} d\mathbf{w}

$$

TODO
- ê·¸ëƒ¥ ìˆ˜ì‹ì ëŠ” ê±¸ë¡œ ì •ë¦¬í•˜ê³  TODO ë‚¨ê²¨ ë†“ê³  chapter 3 ê¹Œì§€ ì½ì€ ë‹¤ìŒ ë‹¤ì‹œ ì½ê¸°
- evidence term ì€ joint distribution ì˜ marginalization ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥
- conjugate prior table ì—ì„œ likelihood ê°€ normal ì´ê³  priorê°€ normalì´ë©´ posteriorë„ normal
  - ê·¸ë ‡ë‹¤ê³  joint distribution ë„ nromal ì´ë¼ê³  í• ìˆ˜ ìˆë‚˜? yes
- $p(\mathbf{w}|\mathsf{x}, \mathsf{t}, \alpha, \beta)$ ë¥¼ ë¨¼ì € êµ¬í•´ì•¼ ê³„ì‚°ì´ ê¹”ë”í•´ì§ˆë“¯..
-

---

ğŸ¢ 1.3 Model Selection
ğŸ¢ 1.4 The Curse of Dimensionality
ğŸ¢ 1.5 Decision Theory
ğŸ¢ 1.5.1 Minimizing the misclassification rate
ğŸ¢ 1.5.2 Minimizing the expected loss
ğŸš€ 1.5.3 The reject option
ğŸ‡ 1.5.4 Inference and decision
ğŸ¢ 1.5.5 Loss functions for regression
ğŸ¢ 1.6 Information Theory
ğŸ¢ 1.6.1 Relative entropy and mutual information

ğŸ¢ 2 Probability Distributions
ğŸ¢ 2.1 Binary Variables
ğŸ¢ 2.1.1 The beta distribution
ğŸ¢ 2.2 Multinomial Variables
ğŸ¢ 2.2.1 The Dirichlet distribution
ğŸ¢ 2.3 The Gaussian Distribution
ğŸ¢ 2.3.1 Conditional Gaussian distributions
ğŸ¢ 2.3.2 Marginal Gaussian distributions
ğŸ¢ 2.3.3 Bayesâ€™ theorem for Gaussian variables
ğŸ¢ 2.3.4 Maximum likelihood for the Gaussian
ğŸ‡ 2.3.5 Sequential estimation
ğŸ‡ 2.3.6 Bayesian inference for the Gaussian
ğŸ‡ 2.3.7 Studentâ€™s t-distribution
ğŸš€ 2.3.8 Periodic variables
ğŸ¢ 2.3.9 Mixtures of Gaussians
ğŸ¢ 2.4 The Exponential Family
ğŸ¢ 2.4.1 Maximum likelihood and sufficient statistics
ğŸ‡ 2.4.2 Conjugate priors
ğŸ‡ 2.4.3 Noninformative priors
ğŸ¢ 2.5 Nonparametric Methods
ğŸ¢ 2.5.1 Kernel density estimators
ğŸ¢ 2.5.2 Nearest-neighbour methods

ğŸ¢ 3 Linear Models for Regression
ğŸ¢ 3.1 Linear Basis Function Models
ğŸ¢ 3.1.1 Maximum likelihood and least squares
ğŸš€ 3.1.2 Geometry of least squares
ğŸ‡ 3.1.3 Sequential learning
ğŸ‡ 3.1.4 Regularized least squares
ğŸš€ 3.1.5 Multiple outputs
ğŸ¢ 3.2 The Bias-Variance Decomposition

---

ğŸ‡ 3.3 Bayesian Linear Regression
ğŸ‡ 3.3.1 Parameter distribution
ğŸ‡ 3.3.2 Predictive distribution
ğŸ‡ 3.3.3 Equivalent kernel

---

ğŸš€ 3.4 Bayesian Model Comparison
ğŸš€ 3.5 The Evidence Approximation
ğŸš€ 3.5.1 Evaluation of the evidence function
ğŸš€ 3.5.2 Maximizing the evidence function
ğŸš€ 3.5.3 Effective number of parameters
ğŸ‡ 3.6 Limitations of Fixed Basis Functions

ğŸ¢ 4 Linear Models for Regression
ğŸ¢ 4.1 Discriminant Functions
ğŸ¢ 4.1.1 Two classes
ğŸ¢ 4.1.2 Multiple classes
ğŸ¢ 4.1.3 Lest squares for classification
ğŸ¢ 4.1.4 Fisherâ€™s linear discriminant
ğŸ¢ 4.1.5 Relation to least squares
ğŸ¢ 4.1.6 Fisherâ€™s discriminant for multiple classes
ğŸ¢ 4.1.7 The perceptron algorithm
ğŸ¢ 4.2 Probabilistic Generative Models
ğŸ¢ 4.2.1 Continuous inputs
ğŸ¢ 4.2.2 Maximum likelihood solution
ğŸ¢ 4.2.3 Discrete features
ğŸ¢ 4.2.4 Exponential family
ğŸ¢ 4.3 Probabilistic Discriminant Models
ğŸ¢ 4.3.1 Fixed basis functions
ğŸ¢ 4.3.2 Logistic regression
ğŸ¢ 4.3.3 Interative reweighted least squares
ğŸ¢ 4.3.4 Multiclass logistic regression
ğŸš€ 4.3.5 Probit regression
ğŸš€ 4.3.6 Canonical link functions
ğŸ‡ 4.4 The Laplace Approximation
ğŸ‡ 4.4.1 Model comparison and BIC
ğŸ‡ 4.5 Bayesian Logistic Regression
ğŸ‡ 4.5.1 Laplace approximation
ğŸ‡ 4.5.2 Predictive distribution

ğŸ¢ 5 Neural Networks
ğŸ¢ 5.1 Feed-forward Networks Functions
ğŸ¢ 5.1.1 Weight-space symmetries
ğŸ¢ 5.2 Network Training
ğŸ¢ 5.2.1 Parameter optimization
ğŸ¢ 5.2.2 Local quadratic approximation
ğŸ¢ 5.2.3 Use of gradient information
ğŸ¢ 5.2.4 Gradient descent optimization
ğŸ¢ 5.3 Error Backpropagation
ğŸ¢ 5.3.1 Evaluation of error-function derivatives
ğŸ¢ 5.3.2 A simple example
ğŸ¢ 5.3.3 Efficiency of backpropagation
ğŸš€ 5.3.4 The Jacobian matrix
ğŸš€ 5.4 The Hessian Matrix
ğŸš€ 5.4.1 Diagonal approximation
ğŸš€ 5.4.2 Outer product approximation
ğŸš€ 5.4.3 Inverse Hessian
ğŸš€ 5.4.4 Finite differences
ğŸš€ 5.4.5 Exact evaluation of the Hessian
ğŸš€ 5.4.6 Fast multiplication by the Hessian
ğŸ‡ 5.5 Regularization in Neural Networks
ğŸ‡ 5.5.1 Consistent Gaussian priors
ğŸ‡ 5.5.2 Early stopping
ğŸš€ 5.5.3 Invariances
ğŸš€ 5.5.4 Tangent propagation
ğŸš€ 5.5.5 Training with transformed data
ğŸš€ 5.5.6 Convolutional networks
ğŸš€ 5.5.7 Soft weight sharing
ğŸš€ 5.6 Mixture Density Networks
ğŸš€ 5.7 Bayesian Neural Networks
ğŸš€ 5.7.1 Posterior parameter distribution
ğŸš€ 5.7.2 Hyperparameter optimization
ğŸš€ 5.7.3 Bayesian neural networks for classification

ğŸ¢ 6 Kernel Methods
ğŸ¢ 6.1 Dual Representaions
ğŸ¢ 6.3 Constructing Kernels
ğŸ¢ 6.3 Radial Basis Function Networks
ğŸ¢ 6.3.1 Nadaraya-Watson model
ğŸš€ 6.4 Gaussian Processes
ğŸš€ 6.4.1 Linear regression revisited
ğŸš€ 6.4.2 Gaussian processes for regression
ğŸš€ 6.4.3 Learning the hyperparameter
ğŸš€ 6.4.4 Automatic relevance determination
ğŸš€ 6.4.5 Gaussian processes for classification
ğŸš€ 6.4.6 Laplace approximation
ğŸš€ 6.4.7 Connection to neural networks

ğŸ¢ 7 Sparse Kernel Machines
ğŸ¢ 7.1 Maximum Margin Classifiers
ğŸ¢ 7.1.1 Overlapping class distributions
ğŸ‡ 7.1.2 Relation to logistic regression
ğŸ‡ 7.1.3 Multiclass SVMs
ğŸ‡ 7.1.4 SVMs for regression
ğŸ‡ 7.1.5 Computational learning theory
ğŸš€ 7.2 Relevance Vector Machines
ğŸš€ 7.2.1 RVM for regression
ğŸš€ 7.2.2 Analysis of sparsity
ğŸš€ 7.2.3 RVM for classification

ğŸ‡ 8 Graphical Models
ğŸ‡ 8.1 Bayesian Networks
ğŸ‡ 8.1.1 Example: Polynomial regression
ğŸ‡ 8.1.2 Generative models
ğŸ‡ 8.1.3 Discrete variables
ğŸ‡ 8.1.4 Linear-Gaussian models
ğŸ‡ 8.2 Conditional Independence
ğŸ‡ 8.2.1 Three example graphs
ğŸ‡ 8.2.2 D-separation
ğŸ‡ 8.3 Markov Random Fields
ğŸ‡ 8.3.1 Conditional independence properties
ğŸ‡ 8.3.2 Factorization properties
ğŸ‡ 8.3.3 Illustration: Image de-noising
ğŸ‡ 8.3.4 Relation to directed graphs
ğŸ‡ 8.4 inference in Graphical Models
ğŸ‡ 8.4.1 Inference on a chain
ğŸ‡ 8.4.2 Trees
ğŸ‡ 8.4.3 Factor graphs
ğŸ‡ 8.4.4 The sum-product algorithm
ğŸ‡ 8.4.5 The max-sum algorithm
ğŸš€ 8.4.6 Exact inference in general graphs
ğŸš€ 8.4.7 Loopy belief propagation
ğŸš€ 8.4.8 Learning the graph structure

ğŸ¢ 9 Mixture Models and EM
ğŸ¢ 9.1 K-means Clustering
ğŸ¢ 9.1.1 Image segmentation and compression
ğŸ¢ 9.2 Mixtures of Gaussians
ğŸ¢ 9.2.1 Maximum likelihood
ğŸ¢ 9.2.2 EM for Gaussian mixtures
ğŸ¢ 9.3 An Alternative View of EM
ğŸ¢ 9.3.1 Gaussian mixtures revisited
ğŸ¢ 9.3.2 Relation to K-means
ğŸš€ 9.3.3 Mixtures of Bernoulli distributions
ğŸš€ 9.3.4 EM for Bayesian linear regression
ğŸ‡ 9.4 The EM Algorithm in General

ğŸ‡ 10 Approximate Inference
ğŸ‡ 10.1 Variational Inference
ğŸ‡ 10.1.1 Factorized distributions
ğŸ‡ 10.1.2 Properties of factorized approximations
ğŸ‡ 10.1.3 Example: The univariate Gaussian
ğŸ‡ 10.1.4 Model comparison
ğŸ‡ 10.2 Illustration: Variational Mixture of Gaussians
ğŸ‡ 10.2.1 Variational distribution
ğŸ‡ 10.2.2 Variational lower bound
ğŸ‡ 10.2.3 Predictive density
ğŸš€ 10.2.4 Determining the number of components
ğŸš€ 10.2.5 Induced factorizations
ğŸš€ 10.3 Variational Linear Regression
ğŸš€ 10.3.1 Variational distribution
ğŸš€ 10.3.2 Predictive distribution
ğŸš€ 10.3.3 Lower bound
ğŸš€ 10.4 Exponential Family Distributions
ğŸš€ 10.4.1 Variational message passing
ğŸš€ 10.5 Local Variational Methods
ğŸš€ 10.6 Variational Logistic Regression
ğŸš€ 10.6.1 Variational posterior distribution
ğŸš€ 10.6.2 Optimizing the variational parameters
ğŸš€ 10.6.3 Inference of hyperparameters
ğŸš€ 10.7 Expectation Propagation
ğŸš€ 10.7.1 Example: The clutter problem
ğŸš€ 10.7.2 Expectation propagation of graphs

ğŸ‡ 11 Sampling Methods
ğŸ‡ 11.1 Basis Sampling Algorithms
ğŸ‡ 11.1.1 Standard distributions
ğŸ‡ 11.1.2 Rejection sampling
ğŸš€ 11.1.3 Adaptive rejection sampling
ğŸš€ 11.1.4 Importance sampling
ğŸš€ 11.1.5 Sampling-importance-resampling
ğŸš€ 11.1.6 Sampling and EM algorithm
ğŸ‡ 11.2 Markov Chain Monte Carlo
ğŸ‡ 11.2.1 Markov chains
ğŸ‡ 11.2.2 The Metropolis-Hastings algorithm
ğŸ‡ 11.3 Gibbs Sampling
ğŸš€ 11.4 Slice Sampling
ğŸš€ 11.5 The Hybrid Monte Carlo Algorithm
ğŸš€ 11.5.1 Dynamical systems
ğŸš€ 11.5.2 Hybrid Monte Carlo
ğŸš€ 11.6 Estimating the Partition Function

ğŸ¢ 12 Continuous Latent Variables
ğŸ¢ 12.1 Principal Component Analysis
ğŸ¢ 12.1.1 Maximum variance formulation
ğŸ¢ 12.1.2 Minimum-error formulation
ğŸ¢ 12.1.3 Applications of peA
ğŸ¢ 12.1.4 PCA for high-dimensional data
ğŸš€ 12.2 Probabilistic p e A
ğŸš€ 12.2.1 Maximum likelihood peA
ğŸš€ 12.2.2 EM algorithm for peA
ğŸš€ 12.2.3 Bayesian peA
ğŸš€ 12.2.4 Factor analysis
ğŸ‡ 12.3 Kernel PCA
ğŸš€ 12.4 Nonliear Latent Variable Models
ğŸš€ 12.4.1 Independent component analysis
ğŸš€ 12.4.2 Autoassociative neural networks
ğŸš€ 12.4.3 Modelling nonlinear manifolds

ğŸ‡ 13 Sequential Data
ğŸ‡ 13.1 Markov Models
ğŸ‡ 13.2 Hidden Markov Models
ğŸ‡ 13.2.1 Maximum likelihood for the HMM
ğŸ‡ 13.2.2 The forward-backward algorithm
ğŸš€ 13.2.3 The sum-product algorithm for the HMM
ğŸš€ 13.2.4 Scaling factors
ğŸ‡ 13.2.5 The Viterbi algorithm
ğŸš€ 13.2.6 Extensions of the hidden Markov model
ğŸ‡ 13.3 Linear Dynamical Systems
ğŸ‡ 13.3.1 Inference in LDS
ğŸ‡ 13.3.2 Learning in LDS
ğŸš€ 13.3.3 Extensions of LDS
ğŸš€ 13.3.4 Particle filters

ğŸ‡ 14 Combining Models
ğŸ‡ 14.1 Bayesian Model Averaging
ğŸ‡ 14.2 Committees
ğŸ‡ 14.3 Boosting
ğŸ‡ 14.3.1 Minimizing exponential error
ğŸ‡ 14.3.2 Error functions for boosting
ğŸ¢ 14.4 Tree-based Models
ğŸš€ 14.5 Conditional Mixture Models
ğŸš€ 14.5.1 Mixtures of linear regression models
ğŸš€ 14.5.2 Mixtures of logistic models
ğŸš€ 14.5.3 Mixtures of experts

## References

- https://dominhhai.github.io/en-us/2017/12/ml-prml/
